{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40630070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torchtext.vocab import GloVe\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bf691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from 'vocab.pkl'.\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB = 25000\n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab_file = 'vocab.pkl'\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(\"Vocabulary loaded from 'vocab.pkl'.\")\n",
    "\n",
    "else:\n",
    "    print(\"no vocab.pkl file found.\")\n",
    "\n",
    "MAX_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032cf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Load GloVe\n",
    "        glove = GloVe(name='6B', dim=100)\n",
    "        pretrained_embeddings = torch.zeros(vocab_size, embed_dim)\n",
    "        for word, idx in vocab.get_stoi().items():\n",
    "            if word in glove.stoi:\n",
    "                pretrained_embeddings[idx] = glove[word]\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False  # freeze\n",
    "\n",
    "        # CNN\n",
    "        self.conv3 = nn.Conv1d(embed_dim, 100, kernel_size=3)\n",
    "        self.conv5 = nn.Conv1d(embed_dim, 100, kernel_size=5)\n",
    "        self.conv7 = nn.Conv1d(embed_dim, 100, kernel_size=7)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Combine and classify\n",
    "        self.fc = nn.Linear(100 * 3 + hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)  # (B, T, E)\n",
    "        x_cnn = x_embed.permute(0, 2, 1)  # (B, E, T)\n",
    "\n",
    "        c3 = torch.relu(self.conv3(x_cnn)).max(dim=2)[0]\n",
    "        c5 = torch.relu(self.conv5(x_cnn)).max(dim=2)[0]\n",
    "        c7 = torch.relu(self.conv7(x_cnn)).max(dim=2)[0]\n",
    "\n",
    "        cnn_out = torch.cat([c3, c5, c7], dim=1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x_embed)\n",
    "        lstm_out = lstm_out[:, -1, :]  # take last timestep\n",
    "\n",
    "        combined = torch.cat([cnn_out, lstm_out], dim=1)\n",
    "        out = self.fc(self.dropout(combined))\n",
    "        return torch.sigmoid(out).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfea4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNN_BiLSTM(vocab_size=len(vocab), embed_dim=100, hidden_dim=128, output_dim=1, pad_idx=vocab['<pad>'])\n",
    "model.to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"model_1.pth\"))\n",
    "model.load_state_dict(torch.load(\"model_1.pth\", map_location=torch.device('cpu')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_news_data.csv')\n",
    "\n",
    "df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e955bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "df = df[df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d372bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 448 texts and 448 labels.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode_df(df):\n",
    "\n",
    "    encoded_texts = []\n",
    "    encoded_labels = []\n",
    "    for text, label in zip(df['content'], df['label']):\n",
    "        if pd.notna(text):\n",
    "            encoded = [vocab[token] for token in tokenizer(text)]\n",
    "            \n",
    "            if len(encoded) <= MAX_LENGTH:\n",
    "                encoded_texts.append(torch.tensor(encoded, dtype=torch.long))\n",
    "                encoded_labels.append(label)\n",
    "\n",
    "    return (\n",
    "        pad_sequence(encoded_texts, batch_first=True, padding_value=vocab['<pad>']),\n",
    "        torch.tensor(encoded_labels, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "encoded_texts, encoded_labels = encode_df(df)\n",
    "print(f\"Encoded {len(encoded_texts)} texts and {len(encoded_labels)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "678764e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae001669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Properly collates batches by stacking tensors\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    return torch.stack(texts), torch.stack(labels)\n",
    "\n",
    "# 2. Create DataLoader with correct collation\n",
    "test_ds = NewsDataset(encoded_texts, encoded_labels)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913b63ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7634\n",
      "Model outputs (probabilities): [[  1]\n",
      " [  2]\n",
      " [  3]\n",
      " [  4]\n",
      " [  5]\n",
      " [  6]\n",
      " [  7]\n",
      " [  9]\n",
      " [ 10]\n",
      " [ 12]\n",
      " [ 15]\n",
      " [ 17]\n",
      " [ 18]\n",
      " [ 19]\n",
      " [ 20]\n",
      " [ 21]\n",
      " [ 22]\n",
      " [ 23]\n",
      " [ 25]\n",
      " [ 26]\n",
      " [ 27]\n",
      " [ 32]\n",
      " [ 33]\n",
      " [ 34]\n",
      " [ 35]\n",
      " [ 36]\n",
      " [ 37]\n",
      " [ 38]\n",
      " [ 39]\n",
      " [ 41]\n",
      " [ 43]\n",
      " [ 44]\n",
      " [ 46]\n",
      " [ 47]\n",
      " [ 48]\n",
      " [ 50]\n",
      " [ 51]\n",
      " [ 52]\n",
      " [ 53]\n",
      " [ 54]\n",
      " [ 55]\n",
      " [ 56]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 61]\n",
      " [ 62]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 74]\n",
      " [ 77]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 84]\n",
      " [ 85]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 90]\n",
      " [ 91]\n",
      " [ 92]\n",
      " [ 93]\n",
      " [ 94]\n",
      " [ 95]\n",
      " [ 96]\n",
      " [ 97]\n",
      " [100]\n",
      " [101]\n",
      " [102]\n",
      " [103]\n",
      " [104]\n",
      " [106]\n",
      " [107]\n",
      " [108]\n",
      " [109]\n",
      " [111]\n",
      " [112]\n",
      " [113]\n",
      " [114]\n",
      " [115]\n",
      " [116]\n",
      " [117]\n",
      " [118]\n",
      " [120]\n",
      " [121]\n",
      " [122]\n",
      " [124]\n",
      " [125]\n",
      " [126]\n",
      " [127]\n",
      " [128]\n",
      " [129]\n",
      " [130]\n",
      " [132]\n",
      " [133]\n",
      " [134]\n",
      " [135]\n",
      " [136]\n",
      " [137]\n",
      " [139]\n",
      " [140]\n",
      " [141]\n",
      " [142]\n",
      " [143]\n",
      " [145]\n",
      " [146]\n",
      " [147]\n",
      " [148]\n",
      " [149]\n",
      " [150]\n",
      " [151]\n",
      " [152]\n",
      " [153]\n",
      " [154]\n",
      " [155]\n",
      " [156]\n",
      " [159]\n",
      " [160]\n",
      " [161]\n",
      " [162]\n",
      " [164]\n",
      " [165]\n",
      " [166]\n",
      " [167]\n",
      " [169]\n",
      " [170]\n",
      " [171]\n",
      " [172]\n",
      " [173]\n",
      " [174]\n",
      " [178]\n",
      " [179]\n",
      " [180]\n",
      " [182]\n",
      " [183]\n",
      " [185]\n",
      " [187]\n",
      " [189]\n",
      " [190]\n",
      " [191]\n",
      " [194]\n",
      " [196]\n",
      " [197]\n",
      " [201]\n",
      " [203]\n",
      " [204]\n",
      " [205]\n",
      " [206]\n",
      " [207]\n",
      " [208]\n",
      " [209]\n",
      " [210]\n",
      " [211]\n",
      " [212]\n",
      " [214]\n",
      " [215]\n",
      " [217]\n",
      " [218]\n",
      " [219]\n",
      " [220]\n",
      " [221]\n",
      " [222]\n",
      " [223]\n",
      " [226]\n",
      " [227]\n",
      " [228]\n",
      " [229]\n",
      " [230]\n",
      " [231]\n",
      " [232]\n",
      " [233]\n",
      " [234]\n",
      " [235]\n",
      " [237]\n",
      " [239]\n",
      " [240]\n",
      " [241]\n",
      " [242]\n",
      " [246]\n",
      " [248]\n",
      " [249]\n",
      " [251]\n",
      " [253]\n",
      " [254]\n",
      " [255]\n",
      " [257]\n",
      " [261]\n",
      " [262]\n",
      " [263]\n",
      " [264]\n",
      " [265]\n",
      " [266]\n",
      " [267]\n",
      " [268]\n",
      " [269]\n",
      " [270]\n",
      " [271]\n",
      " [272]\n",
      " [273]\n",
      " [274]\n",
      " [275]\n",
      " [276]\n",
      " [277]\n",
      " [279]\n",
      " [280]\n",
      " [282]\n",
      " [284]\n",
      " [285]\n",
      " [286]\n",
      " [287]\n",
      " [288]\n",
      " [290]\n",
      " [291]\n",
      " [293]\n",
      " [294]\n",
      " [295]\n",
      " [297]\n",
      " [298]\n",
      " [299]\n",
      " [300]\n",
      " [301]\n",
      " [302]\n",
      " [303]\n",
      " [307]\n",
      " [308]\n",
      " [310]\n",
      " [311]\n",
      " [312]\n",
      " [313]\n",
      " [316]\n",
      " [317]\n",
      " [318]\n",
      " [319]\n",
      " [320]\n",
      " [321]\n",
      " [323]\n",
      " [324]\n",
      " [327]\n",
      " [328]\n",
      " [329]\n",
      " [331]\n",
      " [332]\n",
      " [333]\n",
      " [334]\n",
      " [335]\n",
      " [336]\n",
      " [339]\n",
      " [340]\n",
      " [341]\n",
      " [342]\n",
      " [343]\n",
      " [344]\n",
      " [345]\n",
      " [346]\n",
      " [347]\n",
      " [348]\n",
      " [349]\n",
      " [350]\n",
      " [351]\n",
      " [352]\n",
      " [353]\n",
      " [355]\n",
      " [356]\n",
      " [359]\n",
      " [360]\n",
      " [361]\n",
      " [362]\n",
      " [363]\n",
      " [365]\n",
      " [366]\n",
      " [367]\n",
      " [368]\n",
      " [369]\n",
      " [370]\n",
      " [371]\n",
      " [372]\n",
      " [374]\n",
      " [375]\n",
      " [376]\n",
      " [377]\n",
      " [379]\n",
      " [381]\n",
      " [382]\n",
      " [383]\n",
      " [384]\n",
      " [385]\n",
      " [386]\n",
      " [388]\n",
      " [389]\n",
      " [390]\n",
      " [391]\n",
      " [392]\n",
      " [393]\n",
      " [394]\n",
      " [395]\n",
      " [397]\n",
      " [398]\n",
      " [399]\n",
      " [400]\n",
      " [401]\n",
      " [402]\n",
      " [403]\n",
      " [404]\n",
      " [405]\n",
      " [407]\n",
      " [408]\n",
      " [409]\n",
      " [410]\n",
      " [411]\n",
      " [412]\n",
      " [413]\n",
      " [415]\n",
      " [416]\n",
      " [417]\n",
      " [418]\n",
      " [419]\n",
      " [420]\n",
      " [421]\n",
      " [422]\n",
      " [423]\n",
      " [424]\n",
      " [426]\n",
      " [427]\n",
      " [431]\n",
      " [432]\n",
      " [433]\n",
      " [434]\n",
      " [435]\n",
      " [436]\n",
      " [437]\n",
      " [438]\n",
      " [439]\n",
      " [441]\n",
      " [444]\n",
      " [445]\n",
      " [446]\n",
      " [447]]\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "Ground truth labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# def evaluate(model, loader):\n",
    "#     model.eval()\n",
    "#     total_acc = 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             preds = model(xb)\n",
    "#             preds_class = (preds > 0.5).float()\n",
    "#             total_acc += (preds_class == yb).float().mean().item()\n",
    "#     return total_acc / len(loader)\n",
    "\n",
    "\n",
    "# test_acc = evaluate(model, test_dl)\n",
    "# print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)  # these are the raw outputs (probably sigmoid outputs if binary)\n",
    "            preds_class = (preds > 0.5).float()\n",
    "            \n",
    "            total_acc += (preds_class == yb).float().mean().item()\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "\n",
    "    # concatenate all batches\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return total_acc / len(loader), all_preds, all_labels\n",
    "\n",
    "# then call it like this\n",
    "test_acc, test_preds, test_labels = evaluate(model, test_dl)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# and to actually *print* outputs:\n",
    "print(\"Model outputs (probabilities):\", np.argwhere(test_preds.numpy() <= 0.5))\n",
    "print(encoded_labels[np.argwhere(test_preds.numpy() <= 0.5)])\n",
    "print(\"Ground truth labels:\", test_labels.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs240env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
