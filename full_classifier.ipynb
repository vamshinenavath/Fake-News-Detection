{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3871cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>junksci</td>\n",
       "      <td>First Certified Organic Fast Food Restaurant t...</td>\n",
       "      <td>by ARIANA MARISOL\\n\\nFast food restaurants are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reliable</td>\n",
       "      <td>UP polls: Parties say BJP setting agenda; vote...</td>\n",
       "      <td>India News UP polls: Parties say BJP setting a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>UCLA Student Is Charged With Attempted Murder ...</td>\n",
       "      <td>A UCLA student allegedly stabbed a classmate f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>National Vanguard</td>\n",
       "      <td>Transcript by Katana IT HAS NOW become absolut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>448 Views0 Likes\\n\\nEurope has again been forc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fake</td>\n",
       "      <td>Meet China’s Morlocks: 1 Million Beijing Resid...</td>\n",
       "      <td>Meet China’s Morlocks: 1 Million Beijing Resid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>The Global Elite’s Digital Agenda Played Out a...</td>\n",
       "      <td>Susanne Posel, Contributor\\n\\nActivist Post\\n\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>political</td>\n",
       "      <td>Meet the 99.999999 Percent</td>\n",
       "      <td>Forget about the 1 percent versus the 99 perce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>satire</td>\n",
       "      <td>Isaiah Thomas praises LeBron James</td>\n",
       "      <td>Advertisements\\n\\nAdvertisements\\n\\nEven thoug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bias</td>\n",
       "      <td>Poetry’s Plum Gone to Hell</td>\n",
       "      <td>What is the use of writing about books?” asked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>A Theory On Jesse Ventura and Ron Paul., page 1</td>\n",
       "      <td>edit on 7-4-2011 by edgecrusher2199 because: L...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fake</td>\n",
       "      <td>Nuclear Alert: News Out Of NORTH KOREA… It Is BAD</td>\n",
       "      <td>As Reported By AFF\\n\\nNorth Korea’s nuclear te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>satire</td>\n",
       "      <td>Local Catholic Priests to Hold Traditional Boy...</td>\n",
       "      <td>Local Catholic Priests to Hold Traditional Boy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bias</td>\n",
       "      <td>О грудном молоке нужно позаботиться до беремен...</td>\n",
       "      <td>О грудном молоке нужно позаботиться до беремен...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>Alt Right Archives</td>\n",
       "      <td>The Clinton campaign is so desperate to associ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>political</td>\n",
       "      <td>Germany reports ‘sharp rise’ in cyberattacks</td>\n",
       "      <td>Germany detected a sharp rise in cyberattacks ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>unreliable</td>\n",
       "      <td>Michael Carter Williams</td>\n",
       "      <td>Sports\\n\\nNBA Rumors: Bucks Determined To Trad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>unreliable</td>\n",
       "      <td>Cable: 1975STATE021023</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>unreliable</td>\n",
       "      <td>Cable: 1975VIENNA04470</td>\n",
       "      <td>Tor\\n\\nTor is an encrypted anonymising network...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>satire</td>\n",
       "      <td>When You Are Ready To Have A Serious Conversat...</td>\n",
       "      <td>Larry Groznic\\n\\n\\n\\nI consider you a friend, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                              title  \\\n",
       "0      junksci  First Certified Organic Fast Food Restaurant t...   \n",
       "1     reliable  UP polls: Parties say BJP setting agenda; vote...   \n",
       "2         hate  UCLA Student Is Charged With Attempted Murder ...   \n",
       "4         hate                                  National Vanguard   \n",
       "5   conspiracy                            Truth Broadcast Network   \n",
       "6         fake  Meet China’s Morlocks: 1 Million Beijing Resid...   \n",
       "7   conspiracy  The Global Elite’s Digital Agenda Played Out a...   \n",
       "8    political                         Meet the 99.999999 Percent   \n",
       "9       satire                 Isaiah Thomas praises LeBron James   \n",
       "10        bias                         Poetry’s Plum Gone to Hell   \n",
       "11  conspiracy    A Theory On Jesse Ventura and Ron Paul., page 1   \n",
       "12        fake  Nuclear Alert: News Out Of NORTH KOREA… It Is BAD   \n",
       "13      satire  Local Catholic Priests to Hold Traditional Boy...   \n",
       "14        bias  О грудном молоке нужно позаботиться до беремен...   \n",
       "15   clickbait                                 Alt Right Archives   \n",
       "16   political       Germany reports ‘sharp rise’ in cyberattacks   \n",
       "17  unreliable                            Michael Carter Williams   \n",
       "18  unreliable                             Cable: 1975STATE021023   \n",
       "19  unreliable                             Cable: 1975VIENNA04470   \n",
       "20      satire  When You Are Ready To Have A Serious Conversat...   \n",
       "\n",
       "                                              content  label  \n",
       "0   by ARIANA MARISOL\\n\\nFast food restaurants are...      1  \n",
       "1   India News UP polls: Parties say BJP setting a...      0  \n",
       "2   A UCLA student allegedly stabbed a classmate f...      1  \n",
       "4   Transcript by Katana IT HAS NOW become absolut...      1  \n",
       "5   448 Views0 Likes\\n\\nEurope has again been forc...      1  \n",
       "6   Meet China’s Morlocks: 1 Million Beijing Resid...      1  \n",
       "7   Susanne Posel, Contributor\\n\\nActivist Post\\n\\...      1  \n",
       "8   Forget about the 1 percent versus the 99 perce...      1  \n",
       "9   Advertisements\\n\\nAdvertisements\\n\\nEven thoug...      1  \n",
       "10  What is the use of writing about books?” asked...      1  \n",
       "11  edit on 7-4-2011 by edgecrusher2199 because: L...      1  \n",
       "12  As Reported By AFF\\n\\nNorth Korea’s nuclear te...      1  \n",
       "13  Local Catholic Priests to Hold Traditional Boy...      1  \n",
       "14  О грудном молоке нужно позаботиться до беремен...      1  \n",
       "15  The Clinton campaign is so desperate to associ...      1  \n",
       "16  Germany detected a sharp rise in cyberattacks ...      1  \n",
       "17  Sports\\n\\nNBA Rumors: Bucks Determined To Trad...      1  \n",
       "18  Tor\\n\\nTor is an encrypted anonymising network...      1  \n",
       "19  Tor\\n\\nTor is an encrypted anonymising network...      1  \n",
       "20  Larry Groznic\\n\\n\\n\\nI consider you a friend, ...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"datasets\"\n",
    "file = 'news_traindata' # 'WELFake_Dataset.csv'\n",
    "nrows = 10000\n",
    "vocab_file = f'vocabs/vokab_{file}_{nrows}.pkl'\n",
    "model_file = f\"models/simple_model_{file}_{nrows}.pth\"\n",
    "\n",
    "df = pd.read_csv(f'{dataset_dir}/{file}.csv', encoding='utf-8', nrows=nrows)\n",
    "df = df.dropna()\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc2133",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80485e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from 'vocab.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "MAX_VOCAB = 25000\n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def generate_vocabulary(df):\n",
    "    counter = Counter()\n",
    "    for text in df['content']:\n",
    "        if pd.notna(text):\n",
    "            counter.update(tokenizer(str(text)))\n",
    "    \n",
    "    most_common = [token for token, _ in counter.most_common(MAX_VOCAB - len(special_tokens))]\n",
    "    \n",
    "    vocab = build_vocab_from_iterator([most_common], specials=special_tokens)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "    with open(vocab_file, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(f\"Vocabulary saved to '{vocab_file}'.\")\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(\"Vocabulary loaded from 'vocab.pkl'.\")\n",
    "\n",
    "else:\n",
    "    vocab = generate_vocabulary(df)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered texts: 9668, Labels: torch.Size([9668, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# Label encode the 'type' column\n",
    "le = LabelEncoder()\n",
    "df['type_encoded'] = le.fit_transform(df['type'])\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)  # Number of unique classes\n",
    "\n",
    "encoded_texts_and_labels = []\n",
    "for text, label in zip(df['content'], df['type_encoded']):\n",
    "    if pd.notna(text):\n",
    "        encoded = [vocab[token] for token in tokenizer(text)]\n",
    "        \n",
    "        if len(encoded) <= MAX_LENGTH:\n",
    "            encoded_texts_and_labels.append((torch.tensor(encoded, dtype=torch.long), label))\n",
    "\n",
    "# Separate encoded texts and labels\n",
    "encoded_texts = [item[0] for item in encoded_texts_and_labels]\n",
    "label_indices = torch.tensor([item[1] for item in encoded_texts_and_labels], dtype=torch.long)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "labels = F.one_hot(label_indices, num_classes=NUM_CLASSES).float()\n",
    "\n",
    "# Pad sequences\n",
    "padded_texts = pad_sequence(encoded_texts, batch_first=True, padding_value=vocab['<pad>'])\n",
    "\n",
    "print(f\"Filtered texts: {len(padded_texts)}, Labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735fae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_texts, labels, test_size=0.1)\n",
    "\n",
    "train_ds = NewsDataset(X_train, y_train)\n",
    "val_ds = NewsDataset(X_val, y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee313013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "done constructing model\n",
      "done constructing optimizer\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1447, Val Acc: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.9312, Val Acc: 0.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.8641, Val Acc: 0.6617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.8280, Val Acc: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7975, Val Acc: 0.7087\n"
     ]
    }
   ],
   "source": [
    "from mulstage_model import CNN_BiLSTM\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNN_BiLSTM(vocab=vocab, vocab_size=len(vocab), embed_dim=100, hidden_dim=128, output_dim=NUM_CLASSES, pad_idx=vocab['<pad>'])\n",
    "model.to(device)\n",
    "\n",
    "print(\"done constructing model\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)   \n",
    "\n",
    "print(\"done constructing optimizer\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            # print(preds.shape, yb.shape)\n",
    "            preds_class = preds.argmax(dim=1)\n",
    "            #v print(preds_class)\n",
    "            labels_class = yb.argmax(dim=1)\n",
    "            # print(labels_class)\n",
    "            total_acc += (preds_class == labels_class).float().mean().item()\n",
    "            # print(preds_class, labels_class, total_acc)\n",
    "    return total_acc / len(loader)\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = train(model, train_dl)\n",
    "    acc = evaluate(model, val_dl)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7054c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7678, Val Acc: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.7472, Val Acc: 0.6869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.7309, Val Acc: 0.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.7138, Val Acc: 0.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7025, Val Acc: 0.7147\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), f\"model_{file}_{nrows}.pth\")\n",
    "for epoch in range(5):\n",
    "    loss = train(model, train_dl)\n",
    "    acc = evaluate(model, val_dl)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8bbce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_acc = evaluate(model, val_dl)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs240env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
