{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3871cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e95538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satire</td>\n",
       "      <td>Massive Incoming Comet, Earthquake and Tsunami...</td>\n",
       "      <td>It has been reported that there is a disaster ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>political</td>\n",
       "      <td>A Whisper at the University of Houston-Downtow...</td>\n",
       "      <td>The newscaster said, \"UHD student is trying to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>TV show forces babies to cross-dress, pushes f...</td>\n",
       "      <td>NewsGender\\n\\nISLE OF WIGHT, England, August 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>political</td>\n",
       "      <td>National Review Online</td>\n",
       "      <td>Cliffhanger\\n\\nWright and political wrongs.\\n\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>11 Reasons Why Liberals And Progressives Are M...</td>\n",
       "      <td>13415 SHARES Facebook Twitter Reddit Stumbleup...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>political</td>\n",
       "      <td>Cruz Tells Evangelicals He Can Reverse Marriag...</td>\n",
       "      <td>Cruz Tells Evangelicals He Can Reverse Marriag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unreliable</td>\n",
       "      <td>Cable: 1975BONN06291</td>\n",
       "      <td>Raw content\\n\\nLIMITED OFFICIAL USE PAGE 01 BO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>satire</td>\n",
       "      <td>Champagne renowned for its urine-like flavour,...</td>\n",
       "      <td>Champagne renowned for its urine-like flavour,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hate</td>\n",
       "      <td>Tarik Zahzah 2015-01-30 09-23-55</td>\n",
       "      <td>Click to share on Twitter (Opens in new window)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reliable</td>\n",
       "      <td>NEWS SUMMARY</td>\n",
       "      <td>Nuclear Technology in Iran\\n\\nPresident Mohamm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>Palin-Cowered Media Skewer Michelle Obama</td>\n",
       "      <td>Media Who Are Afraid to Ask Palin What she Rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hate</td>\n",
       "      <td>Rachel Dolezal Isn’t the Only White Person Who...</td>\n",
       "      <td>Rachel Dolezal isn’t the first white person to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unreliable</td>\n",
       "      <td>BreakPoint This Week: The Executive Order, the...</td>\n",
       "      <td>John Stonestreet and Ed Stetzer discuss the di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fake</td>\n",
       "      <td>Spain’s Socialists Reject Rajoy or PP-Led Gove...</td>\n",
       "      <td>Spain’s Socialists Reject Rajoy or PP-Led Gove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bias</td>\n",
       "      <td>CLN : Conscious Life News</td>\n",
       "      <td>FAIR USE NOTICE. Many of the stories on this s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>junksci</td>\n",
       "      <td>Lake Poyang Archives</td>\n",
       "      <td>News\\n\\nThe Bermuda triangle of the East\\n\\nLo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rumor</td>\n",
       "      <td>Star eyes Barcelona move over Real Madrid: He ...</td>\n",
       "      <td>The 19-year-old has made a name for himself in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rumor</td>\n",
       "      <td>Hillary Clinton latest news, pictures and poli...</td>\n",
       "      <td>Hillary Clinton\\n\\nHillary Clinton lost the US...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>junksci</td>\n",
       "      <td>Cosmetics are More than Skin Deep</td>\n",
       "      <td>About the author\\n\\n(NewsTarget) Lotion, deodo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>junksci</td>\n",
       "      <td>renewable resources</td>\n",
       "      <td>(Natural News) Some people who have solar pane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                              title  \\\n",
       "0       satire  Massive Incoming Comet, Earthquake and Tsunami...   \n",
       "1    political  A Whisper at the University of Houston-Downtow...   \n",
       "2    clickbait  TV show forces babies to cross-dress, pushes f...   \n",
       "3    political                             National Review Online   \n",
       "4    clickbait  11 Reasons Why Liberals And Progressives Are M...   \n",
       "5    political  Cruz Tells Evangelicals He Can Reverse Marriag...   \n",
       "6   unreliable                               Cable: 1975BONN06291   \n",
       "7       satire  Champagne renowned for its urine-like flavour,...   \n",
       "8         hate                   Tarik Zahzah 2015-01-30 09-23-55   \n",
       "9     reliable                                       NEWS SUMMARY   \n",
       "10   clickbait          Palin-Cowered Media Skewer Michelle Obama   \n",
       "11        hate  Rachel Dolezal Isn’t the Only White Person Who...   \n",
       "12  unreliable  BreakPoint This Week: The Executive Order, the...   \n",
       "13        fake  Spain’s Socialists Reject Rajoy or PP-Led Gove...   \n",
       "14        bias                          CLN : Conscious Life News   \n",
       "15     junksci                               Lake Poyang Archives   \n",
       "16       rumor  Star eyes Barcelona move over Real Madrid: He ...   \n",
       "17       rumor  Hillary Clinton latest news, pictures and poli...   \n",
       "18     junksci                  Cosmetics are More than Skin Deep   \n",
       "19     junksci                                renewable resources   \n",
       "\n",
       "                                              content  label  \n",
       "0   It has been reported that there is a disaster ...      1  \n",
       "1   The newscaster said, \"UHD student is trying to...      1  \n",
       "2   NewsGender\\n\\nISLE OF WIGHT, England, August 2...      1  \n",
       "3   Cliffhanger\\n\\nWright and political wrongs.\\n\\...      1  \n",
       "4   13415 SHARES Facebook Twitter Reddit Stumbleup...      1  \n",
       "5   Cruz Tells Evangelicals He Can Reverse Marriag...      1  \n",
       "6   Raw content\\n\\nLIMITED OFFICIAL USE PAGE 01 BO...      1  \n",
       "7   Champagne renowned for its urine-like flavour,...      1  \n",
       "8     Click to share on Twitter (Opens in new window)      1  \n",
       "9   Nuclear Technology in Iran\\n\\nPresident Mohamm...      0  \n",
       "10  Media Who Are Afraid to Ask Palin What she Rea...      1  \n",
       "11  Rachel Dolezal isn’t the first white person to...      1  \n",
       "12  John Stonestreet and Ed Stetzer discuss the di...      1  \n",
       "13  Spain’s Socialists Reject Rajoy or PP-Led Gove...      1  \n",
       "14  FAIR USE NOTICE. Many of the stories on this s...      1  \n",
       "15  News\\n\\nThe Bermuda triangle of the East\\n\\nLo...      1  \n",
       "16  The 19-year-old has made a name for himself in...      1  \n",
       "17  Hillary Clinton\\n\\nHillary Clinton lost the US...      1  \n",
       "18  About the author\\n\\n(NewsTarget) Lotion, deodo...      1  \n",
       "19  (Natural News) Some people who have solar pane...      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"datasets\"\n",
    "file = 'news_traindata' # 'WELFake_Dataset.csv'\n",
    "nrows = 110000 * 0.9\n",
    "vocab_file = f'vocabs/vokab_{file}_{nrows}.pkl'\n",
    "model_file = f\"models/simple_model_{file}_{nrows}.pth\"\n",
    "\n",
    "df = pd.read_csv(f'{dataset_dir}/{file}.csv', encoding='utf-8', nrows=nrows)\n",
    "df = df.dropna()\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc2133",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80485e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to 'vocabs/vokab_news_traindata_99000.0.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "MAX_VOCAB = 25000\n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def generate_vocabulary(df):\n",
    "    counter = Counter()\n",
    "    for text in df['content']:\n",
    "        if pd.notna(text):\n",
    "            counter.update(tokenizer(str(text)))\n",
    "    \n",
    "    most_common = [token for token, _ in counter.most_common(MAX_VOCAB - len(special_tokens))]\n",
    "    \n",
    "    vocab = build_vocab_from_iterator([most_common], specials=special_tokens)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "    with open(vocab_file, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(f\"Vocabulary saved to '{vocab_file}'.\")\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(\"Vocabulary loaded from 'vocab.pkl'.\")\n",
    "\n",
    "else:\n",
    "    vocab = generate_vocabulary(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30c9ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 11\n",
      "Filtered texts: 95566, Labels: torch.Size([95566, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# Label encode the 'type' column\n",
    "le = LabelEncoder()\n",
    "df['type_encoded'] = le.fit_transform(df['type'])\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)  # Number of unique classes\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "encoded_texts_and_labels = []\n",
    "for text, label in zip(df['content'], df['type_encoded']):\n",
    "    if pd.notna(text):\n",
    "        encoded = [vocab[token] for token in tokenizer(text)]\n",
    "        \n",
    "        if len(encoded) <= MAX_LENGTH:\n",
    "            encoded_texts_and_labels.append((torch.tensor(encoded, dtype=torch.long), label))\n",
    "\n",
    "# Separate encoded texts and labels\n",
    "encoded_texts = [item[0] for item in encoded_texts_and_labels]\n",
    "label_indices = torch.tensor([item[1] for item in encoded_texts_and_labels], dtype=torch.long)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "labels = F.one_hot(label_indices, num_classes=NUM_CLASSES).float()\n",
    "\n",
    "# Pad sequences\n",
    "padded_texts = pad_sequence(encoded_texts, batch_first=True, padding_value=vocab['<pad>'])\n",
    "\n",
    "print(f\"Filtered texts: {len(padded_texts)}, Labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "735fae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_texts, labels, test_size=0.1)\n",
    "\n",
    "train_ds = NewsDataset(X_train, y_train)\n",
    "val_ds = NewsDataset(X_val, y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee313013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "done constructing model\n",
      "done constructing optimizer\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8776, Val Acc: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.7816, Val Acc: 0.7616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.7560, Val Acc: 0.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.7392, Val Acc: 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7274, Val Acc: 0.7855\n"
     ]
    }
   ],
   "source": [
    "from mulstage_model import CNN_BiLSTM\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNN_BiLSTM(vocab=vocab, vocab_size=len(vocab), embed_dim=100, hidden_dim=128, output_dim=NUM_CLASSES, pad_idx=vocab['<pad>'])\n",
    "model.to(device)\n",
    "\n",
    "print(\"done constructing model\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)   \n",
    "\n",
    "print(\"done constructing optimizer\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            # print(preds.shape, yb.shape)\n",
    "            preds_class = preds.argmax(dim=1)\n",
    "            #v print(preds_class)\n",
    "            labels_class = yb.argmax(dim=1)\n",
    "            # print(labels_class)\n",
    "            total_acc += (preds_class == labels_class).float().mean().item()\n",
    "            # print(preds_class, labels_class, total_acc)\n",
    "    return total_acc / len(loader)\n",
    "\n",
    "print(\"training\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = train(model, train_dl)\n",
    "    acc = evaluate(model, val_dl)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "115bc65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7176, Val Acc: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.7114, Val Acc: 0.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.7034, Val Acc: 0.7939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.6994, Val Acc: 0.7944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.6948, Val Acc: 0.7964\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    loss = train(model, train_dl)\n",
    "    acc = evaluate(model, val_dl)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7054c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_file)\n",
    "# for epoch in range(5):\n",
    "#     loss = train(model, train_dl)\n",
    "#     acc = evaluate(model, val_dl)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs240env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
